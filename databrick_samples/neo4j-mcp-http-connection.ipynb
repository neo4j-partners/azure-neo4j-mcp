{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neo4j MCP Server HTTP Connection Setup\n",
    "\n",
    "This notebook demonstrates how to create a Databricks HTTP connection to the Neo4j MCP server deployed on Azure Container Apps. Once configured, you can query Neo4j graph data directly from SQL using the `http_request` function.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. Validates that secrets are configured in Databricks\n",
    "2. Creates an HTTP connection in Unity Catalog with bearer token authentication\n",
    "3. Tests the connection by calling MCP tools (get-schema, read-cypher)\n",
    "4. Demonstrates how to parse and use the results\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Databricks Runtime**: 15.4 LTS or later, or SQL warehouse 2023.40+\n",
    "- **Unity Catalog**: Must be enabled on your workspace\n",
    "- **Secrets configured**: Run `scripts/setup_databricks_secrets.sh` before this notebook\n",
    "- **MCP server deployed**: The Neo4j MCP server must be running on Azure Container Apps\n",
    "\n",
    "## Security Note\n",
    "\n",
    "This integration provides **READ-ONLY** access to Neo4j. The `write-cypher` tool is intentionally excluded to prevent accidental data modifications from analytics workflows."
   ],
   "id": "markdown-0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Update these values to match your environment. The secret scope should match what you used when running `setup_databricks_secrets.sh`."
   ],
   "id": "markdown-1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration - update these values for your environment\nSECRET_SCOPE = \"mcp-neo4j-secrets\"           # Must match the scope used in setup_databricks_secrets.sh\nCONNECTION_NAME = \"neo4j_azure_beta_mcp\"     # Name for the HTTP connection in Unity Catalog\nCATALOG = \"mcp_demo_catalog\"                  # Unity Catalog catalog name",
   "id": "code-2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Validate Secrets\n",
    "\n",
    "First, verify that the required secrets exist in Databricks. If this step fails, run `scripts/setup_databricks_secrets.sh` from your local machine."
   ],
   "id": "markdown-3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that secrets are configured\n",
    "try:\n",
    "    endpoint = dbutils.secrets.get(scope=SECRET_SCOPE, key=\"endpoint\")\n",
    "    api_key = dbutils.secrets.get(scope=SECRET_SCOPE, key=\"api_key\")\n",
    "    mcp_path = dbutils.secrets.get(scope=SECRET_SCOPE, key=\"mcp_path\")\n",
    "    \n",
    "    print(f\"Secrets validated successfully!\")\n",
    "    print(f\"  Endpoint: {endpoint}\")\n",
    "    print(f\"  MCP Path: {mcp_path}\")\n",
    "    print(f\"  API Key: [REDACTED - {len(api_key)} characters]\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to retrieve secrets from scope '{SECRET_SCOPE}'\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\")\n",
    "    print(\"To fix this, run the setup script from your local machine:\")\n",
    "    print(f\"  ./scripts/setup_databricks_secrets.sh {SECRET_SCOPE}\")\n",
    "    raise"
   ],
   "id": "code-4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Create the HTTP Connection\n\nCreate an HTTP connection in Unity Catalog. This connection:\n- Points to the MCP server endpoint\n- Uses bearer token authentication with the API key from secrets\n- Can be shared with other users via Unity Catalog permissions\n\n**Implementation Note**: We use `spark.sql()` with Python f-strings instead of `%%sql` magic because the `CREATE CONNECTION` OPTIONS clause requires constant expressions. Python string interpolation allows us to dynamically insert the host, path, and secret scope values.\n\n**MCP Integration Note**: For full Databricks AI/MCP integration features, you may need to manually enable the \"Is mcp connection\" checkbox in the Catalog Explorer UI after creating the connection. Navigate to: **Catalog > External Data > Connections > neo4j_mcp > Edit** and check the MCP option. This setting is not currently available via SQL DDL.\n\n**Note**: If the connection already exists, you'll see an error. Use the cleanup cell at the bottom to drop it first, then re-run this cell.",
   "id": "markdown-5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get connection parameters from secrets\nendpoint = dbutils.secrets.get(scope=SECRET_SCOPE, key=\"endpoint\")\nmcp_path = dbutils.secrets.get(scope=SECRET_SCOPE, key=\"mcp_path\")\n\n# Use full endpoint URL including https:// as the host\n# (Databricks HTTP connections expect the full URL with protocol)\nhost = endpoint if endpoint.startswith(\"https://\") else f\"https://{endpoint}\"\n\nprint(f\"Creating HTTP connection '{CONNECTION_NAME}'...\")\nprint(f\"  Host: {host}\")\nprint(f\"  Port: 443\")\nprint(f\"  Base Path: {mcp_path}\")\nprint(f\"  Secret Scope: {SECRET_SCOPE}\")",
   "id": "code-6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql1"
    }
   },
   "outputs": [],
   "source": "# Create the HTTP connection with bearer token authentication\n# The bearer_token references the secret, so the actual key is never exposed\n# Note: Using spark.sql() because OPTIONS requires constant expressions (no variable interpolation in %%sql)\n\n# Use catalog-qualified name for the connection\nfull_connection_name = f\"{CATALOG}.{CONNECTION_NAME}\"\n\ncreate_connection_sql = f\"\"\"\nCREATE CONNECTION IF NOT EXISTS {full_connection_name} TYPE HTTP\nOPTIONS (\n  host '{host}',\n  port '443',\n  base_path '{mcp_path}',\n  bearer_token secret ('{SECRET_SCOPE}', 'api_key')\n)\n\"\"\"\n\nprint(\"Executing SQL:\")\nprint(create_connection_sql)\n\nspark.sql(create_connection_sql)\nprint(f\"\\nConnection created: {full_connection_name}\")",
   "id": "code-7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify connection was created\nprint(f\"Connection '{full_connection_name}' created successfully!\")\nprint(\"\")\nprint(f\"You can now use this connection with http_request() in SQL.\")\nprint(f\"\")\nprint(f\"To use in Databricks Playground:\")\nprint(f\"  1. Go to Add tools > MCP Servers tab\")\nprint(f\"  2. Under 'External MCP Servers', select '{full_connection_name}' from the Unity Catalog Connection dropdown\")",
   "id": "code-8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test the Connection - List Tools\n",
    "\n",
    "The MCP protocol uses JSON-RPC. Let's first list the available tools to verify the connection works."
   ],
   "id": "markdown-9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# MCP JSON-RPC request to list tools\n",
    "list_tools_request = json.dumps({\n",
    "    \"jsonrpc\": \"2.0\",\n",
    "    \"method\": \"tools/list\",\n",
    "    \"id\": 1\n",
    "})\n",
    "\n",
    "print(\"Request payload:\")\n",
    "print(list_tools_request)"
   ],
   "id": "code-10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql2"
    }
   },
   "outputs": [],
   "source": "# List available MCP tools\n# Note: Using Python to reference the full_connection_name variable\nlist_tools_sql = f\"\"\"\nSELECT http_request(\n  conn => '{full_connection_name}',\n  method => 'POST',\n  path => '',\n  headers => map('Content-Type', 'application/json'),\n  json => '{{\"jsonrpc\":\"2.0\",\"method\":\"tools/list\",\"id\":1}}'\n) AS response\n\"\"\"\n\nresult = spark.sql(list_tools_sql)\ndisplay(result)",
   "id": "code-11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Get Neo4j Schema\n",
    "\n",
    "Call the `get-schema` tool to retrieve the Neo4j database schema, including node labels, relationship types, and properties."
   ],
   "id": "markdown-12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql3"
    }
   },
   "outputs": [],
   "source": "# Get the Neo4j database schema\nget_schema_sql = f\"\"\"\nSELECT http_request(\n  conn => '{full_connection_name}',\n  method => 'POST',\n  path => '',\n  headers => map('Content-Type', 'application/json'),\n  json => '{{\"jsonrpc\":\"2.0\",\"method\":\"tools/call\",\"params\":{{\"name\":\"get-schema\",\"arguments\":{{}}}},\"id\":2}}'\n) AS response\n\"\"\"\n\nresult = spark.sql(get_schema_sql)\ndisplay(result)",
   "id": "code-13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Execute a Read Query\n",
    "\n",
    "Call the `read-cypher` tool to execute a read-only Cypher query against Neo4j.\n",
    "\n",
    "**Important**: Only read queries are permitted through this connection. The `write-cypher` tool is intentionally not exposed."
   ],
   "id": "markdown-14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Example: Count nodes by label\n",
    "cypher_query = \"MATCH (n) RETURN labels(n) AS label, count(*) AS count ORDER BY count DESC LIMIT 10\"\n",
    "\n",
    "# Build the MCP request\n",
    "read_cypher_request = json.dumps({\n",
    "    \"jsonrpc\": \"2.0\",\n",
    "    \"method\": \"tools/call\",\n",
    "    \"params\": {\n",
    "        \"name\": \"read-cypher\",\n",
    "        \"arguments\": {\n",
    "            \"query\": cypher_query\n",
    "        }\n",
    "    },\n",
    "    \"id\": 3\n",
    "})\n",
    "\n",
    "print(\"Cypher query:\")\n",
    "print(cypher_query)\n",
    "print(\"\")\n",
    "print(\"MCP request:\")\n",
    "print(read_cypher_request)"
   ],
   "id": "code-15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the request for use in SQL\n",
    "spark.conf.set(\"cypher_request\", read_cypher_request)"
   ],
   "id": "code-16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql4"
    }
   },
   "outputs": [],
   "source": "# Execute a read-only Cypher query\n# Escape curly braces for f-string and single quotes for SQL\nescaped_request = read_cypher_request.replace(\"'\", \"''\")\n\ncypher_sql = f\"\"\"\nSELECT http_request(\n  conn => '{full_connection_name}',\n  method => 'POST',\n  path => '',\n  headers => map('Content-Type', 'application/json'),\n  json => '{escaped_request}'\n) AS response\n\"\"\"\n\nresult = spark.sql(cypher_sql)\ndisplay(result)",
   "id": "code-17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Parse and Display Results\n",
    "\n",
    "The MCP response is JSON. Let's parse it and display the results in a more readable format."
   ],
   "id": "markdown-18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql.functions import col, from_json, get_json_object\nfrom pyspark.sql.types import StringType, StructType, StructField, ArrayType\n\n# Execute the query and get the response\nescaped_request = read_cypher_request.replace(\"'\", \"''\")\n\nresult_df = spark.sql(f\"\"\"\n    SELECT http_request(\n      conn => '{full_connection_name}',\n      method => 'POST',\n      path => '',\n      headers => map('Content-Type', 'application/json'),\n      json => '{escaped_request}'\n    ) AS response\n\"\"\")\n\n# Extract the response body\nresponse_row = result_df.first()\nif response_row:\n    response = response_row[\"response\"]\n    print(\"Raw response:\")\n    print(json.dumps(json.loads(response[\"text\"]), indent=2) if \"text\" in response else response)",
   "id": "code-19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Query Neo4j\n",
    "\n",
    "Here's a reusable function to query Neo4j through the MCP connection."
   ],
   "id": "markdown-20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\n\ndef query_neo4j(cypher_query: str, connection_name: str = None) -> dict:\n    \"\"\"\n    Execute a read-only Cypher query against Neo4j via the MCP HTTP connection.\n    \n    Args:\n        cypher_query: The Cypher query to execute (read-only)\n        connection_name: Full name of the HTTP connection (default: uses full_connection_name from notebook)\n    \n    Returns:\n        dict: The parsed JSON response from Neo4j\n    \"\"\"\n    if connection_name is None:\n        connection_name = full_connection_name\n    \n    # Build the MCP request\n    request_payload = json.dumps({\n        \"jsonrpc\": \"2.0\",\n        \"method\": \"tools/call\",\n        \"params\": {\n            \"name\": \"read-cypher\",\n            \"arguments\": {\n                \"query\": cypher_query\n            }\n        },\n        \"id\": 1\n    })\n    \n    # Escape single quotes for SQL\n    escaped_payload = request_payload.replace(\"'\", \"''\")\n    \n    # Execute the query\n    result_df = spark.sql(f\"\"\"\n        SELECT http_request(\n          conn => '{connection_name}',\n          method => 'POST',\n          path => '',\n          headers => map('Content-Type', 'application/json'),\n          json => '{escaped_payload}'\n        ) AS response\n    \"\"\")\n    \n    # Parse the response\n    response_row = result_df.first()\n    if response_row and \"response\" in response_row.asDict():\n        response = response_row[\"response\"]\n        if \"text\" in response:\n            return json.loads(response[\"text\"])\n    return None\n\nprint(\"Helper function 'query_neo4j' is now available.\")\nprint(\"\")\nprint(\"Example usage:\")\nprint(f'  result = query_neo4j(\"MATCH (n) RETURN count(n) AS total\")')\nprint(f\"  # Uses connection: {full_connection_name}\")",
   "id": "code-21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Use the helper function\n",
    "result = query_neo4j(\"MATCH (n) RETURN count(n) AS total\")\n",
    "print(\"Query result:\")\n",
    "print(json.dumps(result, indent=2))"
   ],
   "id": "code-22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "**1. Secret not found**\n",
    "```\n",
    "Secret does not exist with scope: mcp-neo4j-secrets and key: api_key\n",
    "```\n",
    "Solution: Run `./scripts/setup_databricks_secrets.sh` from your local machine.\n",
    "\n",
    "**2. Connection already exists**\n",
    "```\n",
    "Connection 'neo4j_mcp' already exists\n",
    "```\n",
    "Solution: Use the cleanup cell below to drop the connection, then re-create it.\n",
    "\n",
    "**3. HTTP request timeout**\n",
    "```\n",
    "Connection timed out\n",
    "```\n",
    "Solution: Verify the MCP server is running. Check the endpoint URL in your secrets.\n",
    "\n",
    "**4. Authentication failed**\n",
    "```\n",
    "401 Unauthorized\n",
    "```\n",
    "Solution: The API key may be incorrect. Re-run `setup_databricks_secrets.sh` to refresh the secrets from `MCP_ACCESS.json`."
   ],
   "id": "markdown-23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Use this cell to drop the HTTP connection if you need to recreate it or clean up resources."
   ],
   "id": "markdown-24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Uncomment the lines below to drop the connection\n# spark.sql(f\"DROP CONNECTION IF EXISTS {full_connection_name}\")\n# print(f\"Connection '{full_connection_name}' dropped.\")",
   "id": "code-25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you have a working HTTP connection to Neo4j:\n",
    "\n",
    "1. **Explore the schema**: Use `get-schema` to understand your graph structure\n",
    "2. **Write analytics queries**: Use `read-cypher` to query graph data for your analytics\n",
    "3. **Join with Delta tables**: Combine graph query results with your Delta Lake data\n",
    "4. **Share the connection**: Grant `USE CONNECTION` to other users via Unity Catalog\n",
    "\n",
    "For more information:\n",
    "- [Databricks HTTP Connections](https://docs.databricks.com/aws/en/query-federation/http)\n",
    "- [Neo4j Cypher Query Language](https://neo4j.com/docs/cypher-manual/current/)\n",
    "- [Model Context Protocol (MCP)](https://modelcontextprotocol.io/)"
   ],
   "id": "markdown-26"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}