{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-markdown-1",
   "metadata": {},
   "source": "# Neo4j MCP Agent: Test, Evaluate & Deploy\n\nThis notebook tests and deploys the Neo4j MCP tool-calling LangGraph agent that connects to a Neo4j graph database through an external MCP server hosted on Azure Container Apps.\n\n## What This Notebook Does\n\n1. **Test the agent** - Verify the agent can query Neo4j via MCP tools\n2. **Log as MLflow model** - Package the agent for deployment\n3. **Evaluate with Agent Evaluation** - Assess quality with MLflow scorers\n4. **Register to Unity Catalog** - Store the model in UC for governance\n5. **Deploy to Model Serving** - Create a serving endpoint\n\n## Prerequisites\n\n- **HTTP Connection**: `neo4j_azure_beta_mcp` created (see `neo4j-mcp-http-connection.ipynb`)\n- **MCP Flag Enabled**: \"Is MCP connection\" checkbox checked in Catalog Explorer\n- **Secrets Configured**: Run `scripts/setup_databricks_secrets.sh` first\n- **Neo4j MCP Server**: Running on Azure Container Apps\n\n## Architecture\n\n```\n┌─────────────────┐     ┌──────────────────────┐     ┌─────────────────┐     ┌─────────────────┐\n│  LangGraph      │────▶│ Unity Catalog HTTP   │────▶│ Neo4j MCP Server│────▶│ Neo4j Database  │\n│  Agent          │     │ Connection Proxy     │     │ (Azure)         │     │                 │\n│                 │     │ /api/2.0/mcp/external│     │                 │     │                 │\n└─────────────────┘     └──────────────────────┘     └─────────────────┘     └─────────────────┘\n       MCP tool calls           Bearer Token              Cypher Queries\n       (JSON-RPC)               from Secrets              (read-cypher tool)\n```"
  },
  {
   "cell_type": "markdown",
   "id": "setup-markdown-2",
   "metadata": {},
   "source": "## Setup\n\n**Important:** Before running this notebook, ensure your cluster is configured with the required libraries.\n\nSee the [README.md](./README.md#cluster-setup) for cluster setup instructions, including:\n- Enabling **Machine Learning** runtime (17.3 LTS ML or later)\n- Installing required PyPI packages: `databricks-agents`, `databricks-langchain`, `langgraph`, `mcp`, `databricks-mcp`\n\nThe cluster must be restarted after installing libraries."
  },
  {
   "cell_type": "markdown",
   "id": "test-header-5",
   "metadata": {},
   "source": [
    "## Test the Agent\n",
    "\n",
    "Import the agent from `neo4j_mcp_agent.py` and test its tool-calling abilities.\n",
    "Since the agent uses `mlflow.langchain.autolog()`, you can view traces in the MLflow UI."
   ]
  },
  {
   "cell_type": "code",
   "id": "import-agent-6",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neo4j_mcp_agent import AGENT, CONNECTION_NAME, SECRET_SCOPE\n",
    "\n",
    "print(f\"Agent loaded successfully!\")\n",
    "print(f\"Using HTTP connection: {CONNECTION_NAME}\")\n",
    "print(f\"Using secrets scope: {SECRET_SCOPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-schema-7",
   "metadata": {},
   "source": [
    "### Test 1: Get Database Schema\n",
    "\n",
    "Ask the agent to retrieve the Neo4j database schema using the `get-schema` tool."
   ]
  },
  {
   "cell_type": "code",
   "id": "schema-test-8",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test get-schema tool\n",
    "response = AGENT.predict({\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"What is the schema of the Neo4j database? Show me the node labels and relationship types.\"}]\n",
    "})\n",
    "print(\"Schema Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-query-9",
   "metadata": {},
   "source": [
    "### Test 2: Execute a Cypher Query\n",
    "\n",
    "Ask the agent to count nodes by label using the `read-cypher` tool."
   ]
  },
  {
   "cell_type": "code",
   "id": "query-test-10",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test read-cypher tool\n",
    "response = AGENT.predict({\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"How many nodes are there in the database? Break it down by node label.\"}]\n",
    "})\n",
    "print(\"Query Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stream-header-11",
   "metadata": {},
   "source": [
    "### Test 3: Streaming Response\n",
    "\n",
    "Test the streaming capability to see the agent's thought process in real-time."
   ]
  },
  {
   "cell_type": "code",
   "id": "stream-test-12",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test streaming\n",
    "print(\"Streaming response:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for chunk in AGENT.predict_stream(\n",
    "    {\"input\": [{\"role\": \"user\", \"content\": \"What relationships exist in the database? List the top 5 by count.\"}]}\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "log-header-13",
   "metadata": {},
   "source": [
    "## Log the Agent as an MLflow Model\n",
    "\n",
    "Log the agent as code from `neo4j_mcp_agent.py`. This packages the agent with its dependencies for deployment.\n",
    "\n",
    "See [Deploy an agent that connects to Databricks MCP servers](https://docs.databricks.com/aws/en/generative-ai/mcp/managed-mcp#deploy-your-agent)."
   ]
  },
  {
   "cell_type": "code",
   "id": "log-model-14",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from neo4j_mcp_agent import LLM_ENDPOINT_NAME, CONNECTION_NAME\n",
    "from mlflow.models.resources import DatabricksServingEndpoint, DatabricksUCConnection\n",
    "from pkg_resources import get_distribution\n",
    "\n",
    "# Define resources the agent depends on\n",
    "resources = [\n",
    "    DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME),\n",
    "    DatabricksUCConnection(connection_name=CONNECTION_NAME),\n",
    "]\n",
    "\n",
    "# Log the agent model\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"neo4j-mcp-agent\",\n",
    "        python_model=\"neo4j_mcp_agent.py\",\n",
    "        resources=resources,\n",
    "        pip_requirements=[\n",
    "            f\"langgraph=={get_distribution('langgraph').version}\",\n",
    "            f\"mcp=={get_distribution('mcp').version}\",\n",
    "            f\"databricks-mcp=={get_distribution('databricks-mcp').version}\",\n",
    "            f\"databricks-langchain=={get_distribution('databricks-langchain').version}\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(f\"Model logged successfully!\")\n",
    "print(f\"Run ID: {logged_agent_info.run_id}\")\n",
    "print(f\"Model URI: {logged_agent_info.model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-header-15",
   "metadata": {},
   "source": [
    "## Evaluate the Agent with Agent Evaluation\n",
    "\n",
    "Evaluate the agent using [MLflow Agent Evaluation](https://docs.databricks.com/mlflow3/genai/eval-monitor).\n",
    "This uses predefined LLM scorers to assess response quality and safety.\n",
    "\n",
    "You can customize the evaluation dataset and add [custom scorers](https://docs.databricks.com/mlflow3/genai/eval-monitor/custom-scorers)."
   ]
  },
  {
   "cell_type": "code",
   "id": "eval-dataset-16",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.scorers import RelevanceToQuery, Safety\n",
    "\n",
    "# Define evaluation dataset with Neo4j-specific queries\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"input\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"What node labels exist in the Neo4j database?\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"expected_response\": \"The agent should call get-schema and return a list of node labels from the database.\"\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"input\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Count all nodes in the database\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"expected_response\": \"The agent should execute a Cypher query like MATCH (n) RETURN count(n) and return the total count.\"\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"input\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"What are the most common relationship types?\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"expected_response\": \"The agent should query relationship types and their counts, returning the most common ones.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Evaluation dataset contains {len(eval_dataset)} test cases\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "run-eval-17",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "eval_results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=lambda input: AGENT.predict({\"input\": input}),\n",
    "    scorers=[RelevanceToQuery(), Safety()],\n",
    ")\n",
    "\n",
    "print(\"Evaluation complete! Review results in the MLflow UI.\")\n",
    "print(f\"Results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predict-header-18",
   "metadata": {},
   "source": [
    "### Test the Logged Model\n",
    "\n",
    "Verify the logged model works by running a prediction using `mlflow.models.predict`."
   ]
  },
  {
   "cell_type": "code",
   "id": "test-logged-19",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test the logged model\n",
    "result = mlflow.models.predict(\n",
    "    model_uri=f\"runs:/{logged_agent_info.run_id}/neo4j-mcp-agent\",\n",
    "    input_data={\"input\": [{\"role\": \"user\", \"content\": \"How many nodes are in the database?\"}]},\n",
    "    env_manager=\"uv\",\n",
    ")\n",
    "\n",
    "print(\"Logged model prediction:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "register-header-20",
   "metadata": {},
   "source": "## Register the Model to Unity Catalog\n\nBefore deploying, register the agent to Unity Catalog for governance and versioning.\n\nSee [README.md](./README.md#usage) for instructions on creating the catalog and schema if needed."
  },
  {
   "cell_type": "code",
   "id": "register-model-22",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Set Unity Catalog as the model registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Register the model\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri,\n",
    "    name=UC_MODEL_NAME\n",
    ")\n",
    "\n",
    "print(f\"Model registered successfully!\")\n",
    "print(f\"Name: {uc_registered_model_info.name}\")\n",
    "print(f\"Version: {uc_registered_model_info.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deploy-header-23",
   "metadata": {},
   "source": [
    "## Deploy the Agent\n",
    "\n",
    "Deploy the registered model to a Databricks Model Serving endpoint.\n",
    "\n",
    "This creates a REST API endpoint that can be called to interact with the Neo4j MCP agent."
   ]
  },
  {
   "cell_type": "code",
   "id": "deploy-agent-24",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "\n",
    "# Deploy the agent\n",
    "deployment = agents.deploy(\n",
    "    UC_MODEL_NAME,\n",
    "    uc_registered_model_info.version,\n",
    "    tags={\"endpointSource\": \"neo4j-mcp\", \"connection\": CONNECTION_NAME},\n",
    "    deploy_feedback_model=False\n",
    ")\n",
    "\n",
    "print(f\"Deployment initiated!\")\n",
    "print(f\"Endpoint: {deployment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usage-header-25",
   "metadata": {},
   "source": [
    "## Usage Examples\n",
    "\n",
    "Once deployed, you can call the agent endpoint using the Databricks SDK or REST API."
   ]
  },
  {
   "cell_type": "code",
   "id": "usage-example-26",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Example: Query the deployed endpoint\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Get the endpoint name from deployment\n",
    "# endpoint_name = deployment.endpoint_name  # Uncomment after deployment\n",
    "\n",
    "# Example query structure\n",
    "example_query = {\n",
    "    \"input\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is the structure of the Neo4j database?\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Example query format:\")\n",
    "print(example_query)\n",
    "print(\"\")\n",
    "print(\"To query the endpoint:\")\n",
    "print(\"\")\n",
    "print(\"from databricks.sdk import WorkspaceClient\")\n",
    "print(\"w = WorkspaceClient()\")\n",
    "print(f\"response = w.serving_endpoints.query(name='<endpoint_name>', inputs=example_query)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-header-27",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Uncomment the cells below to delete resources when no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "id": "cleanup-code-28",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Uncomment to delete the serving endpoint\n",
    "# from databricks.sdk import WorkspaceClient\n",
    "# w = WorkspaceClient()\n",
    "# w.serving_endpoints.delete(name=\"<endpoint_name>\")\n",
    "# print(\"Endpoint deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps-29",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Customize the system prompt** - Edit `neo4j_mcp_agent.py` to tailor the agent's behavior\n",
    "2. **Add more evaluation cases** - Expand the eval dataset with domain-specific queries\n",
    "3. **Monitor in production** - Use MLflow to track agent performance and latency\n",
    "4. **Share access** - Grant permissions to the serving endpoint for other users\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [External MCP Servers Documentation](https://docs.databricks.com/aws/en/generative-ai/mcp/external-mcp)\n",
    "- [MLflow Agent Evaluation](https://docs.databricks.com/mlflow3/genai/eval-monitor)\n",
    "- [Databricks Model Serving](https://docs.databricks.com/aws/en/machine-learning/model-serving/)\n",
    "- [Neo4j Cypher Manual](https://neo4j.com/docs/cypher-manual/current/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
